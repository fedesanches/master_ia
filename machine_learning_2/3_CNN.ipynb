{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7n1imMFNTqcJ"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEYjn2EtTrxm"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10):\n",
        "        \"\"\"\n",
        "        Define the layers of the convolutional neural network.\n",
        "\n",
        "        Parameters:\n",
        "            in_channels: int\n",
        "                The number of channels in the input image. For MNIST, this is 1 (grayscale images).\n",
        "            num_classes: int\n",
        "                The number of classes we want to predict, in our case 10 (digits 0 to 9).\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer: 1 input channel, 8 output channels (filtros), 3x3 kernel, stride 1, padding 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Max pooling layer: 2x2 window, stride 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Second convolutional layer: 8 input channels, 16 output channels, 3x3 kernel, stride 1, padding 1\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        # Fully connected layer: 16*7*7 input features (after two 2x2 poolings), 10 output features (num_classes)\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass of the neural network.\n",
        "\n",
        "        Parameters:\n",
        "            x: torch.Tensor\n",
        "                The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor\n",
        "                The output tensor after passing through the network.\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
        "        x = self.pool(x)           # Apply max pooling\n",
        "        x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
        "        x = self.pool(x)           # Apply max pooling\n",
        "        x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n",
        "        x = self.fc1(x)            # Apply fully connected layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eY2J8SHxTrvA"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ijXVyLppTrsC"
      },
      "outputs": [],
      "source": [
        "input_size = 784  # 28x28 pixels (not directly used in CNN)\n",
        "num_classes = 10  # digits 0-9\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10  # Reduced for demonstration purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeE5R0oGTrpT",
        "outputId": "a34f145d-e6b9-40e8-c5ed-0633914beed9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 340kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.16MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.00MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=False, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "WO-WtmnWWhl9",
        "outputId": "16f458c0-00bf-4c0a-fb32-e88c8c86c253"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the first image and label from the training dataset\n",
        "image, label = train_dataset[0]\n",
        "\n",
        "# Permute the dimensions of the image tensor to (height, width, channels) for plotting\n",
        "image = image.permute(1, 2, 0)\n",
        "\n",
        "# Set the figure size to make the image smaller (e.g., 2x2 inches)\n",
        "plt.figure(figsize=(2, 2))\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(image.squeeze(), cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off')  # Hide the axes\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MMQPQnLvTrmK"
      },
      "outputs": [],
      "source": [
        "model = CNN(in_channels=1, num_classes=num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SOoii0ncTrjc"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkrMY9TTrg9",
        "outputId": "e52931f7-ac35-4fc1-93ad-0ee8321a5171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:24<00:00, 38.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 40.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 40.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 41.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 40.34it/s]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "    for batch_index, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "        # Move data and targets to the device (GPU/CPU)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass: compute the model output\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward pass: compute the gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimization step: update the model parameters\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4kQjfPTPTrdU"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model):\n",
        "    \"\"\"\n",
        "    Checks the accuracy of the model on the given dataset loader.\n",
        "\n",
        "    Parameters:\n",
        "        loader: DataLoader\n",
        "            The DataLoader for the dataset to check accuracy on.\n",
        "        model: nn.Module\n",
        "            The neural network model.\n",
        "    \"\"\"\n",
        "    if loader.dataset.train:\n",
        "        print(\"Checking accuracy on training data\")\n",
        "    else:\n",
        "        print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)  # Get the index of the max log-probability\n",
        "            num_correct += (predictions == y).sum()  # Count correct predictions\n",
        "            num_samples += predictions.size(0)  # Count total samples\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = float(num_correct) / float(num_samples) * 100\n",
        "        print(f\"Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%\")\n",
        "\n",
        "    model.train()  # Set the model back to training mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZmFXW2CTrak",
        "outputId": "d97d9885-55a3-4b62-eba5-83b30e46fdf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking accuracy on training data\n",
            "Got 59508/60000 with accuracy 99.18%\n",
            "Checking accuracy on test data\n",
            "Got 9853/10000 with accuracy 98.53%\n"
          ]
        }
      ],
      "source": [
        "# Final accuracy check on training and test sets\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBwlb3fAWA71"
      },
      "source": [
        "Ejercicios:\n",
        "1) Graficar learning curves.\n",
        "2) Comparar performance para distinto número de capas convolucionales.\n",
        "3) Preprocesar el input para que las imagenes sean 1 o 0 (blanco o negro, no gris)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFvaBE61TrX8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
